quarkus.langchain4j.ollama.chat-model.model-id=llama3.2:latest
quarkus.langchain4j.ollama.timeout=60s
quarkus.langchain4j.ollama.chat-model.temperature=0.5
quarkus.langchain4j.ollama.chat-model.top-p=0.9

quarkus.langchain4j.chat-memory.token-window.max-tokens=500
quarkus.devservices.enabled=false
quarkus.devui.enable=false
